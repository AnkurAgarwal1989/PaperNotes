{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For BA to solve the Non-linear optimization problem of SLAM, the following need to exist:\n",
    "1. Corresponding observations among selected keyframes.\n",
    "2. A method of keyframe selection; avoid unnecessay redundancy.\n",
    "3. A strong network of keyframes observing points with significant parallax.\n",
    "4. Initial estimate of keyframe poses and point locations.\n",
    "5. Optimization focussed in a local map; to provide scalability.\n",
    "6. Fast global optimization for real time loop closure.\n",
    "\n",
    "## Main Features of ORB-SLAM:\n",
    "1. Using **_same_** features (ORB) for __Tracking__, __Mapping__, __Reloc__ and __Loop Closure__; efficient and reliable.\n",
    "2. __Survival of fittest__ approach for keyframe selection; generous spawing, restrictive culling.\n",
    "* __Essential Graph__; loop closure links, strong edges from covisibility graph; real time loop closing.\n",
    "* Automatic and robus initialization; model selection, initial map from planar and non-planar scenes.\n",
    "* __Covisibility Graph__ for tracking and mapping in a local covisible area; real time operation.\n",
    "* Real time __Camera Relocalization__; allows better recovery from tracking failure, enhances map reuse.\n",
    "\n",
    "\n",
    "## Optimization Objective:\n",
    "1. Non-linear optimization of **Feature Reprojection Errors**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "__A. Feature Choice:__\n",
    "  * Same features for mapping, tracking, reloc and loop closure.\n",
    "  * Features need to be extracted in < 33ms; can;t do SIFT, SURF, A-KAZE\n",
    "  * Need to be rotation invariant to support place recog; can't do BRIEF, LDB\n",
    "  * Use ORB (Oriented multi-scale FAST, 256-bit descriptor\n",
    "    + Extremely fast\n",
    "    + Viewpoint invariant\n",
    "\n",
    "__B. 3 Threads; Tracking, Local Mapping and Loop Closure:__\n",
    "  1. Tracking thread:\n",
    "    * Localization of camera with every frame, __decide if keyframe__\n",
    "    * Feature matching with previous frame, optimization with motion-only BA.\n",
    "    * If tracking is lost, place recog module used for global reloc.\n",
    "    * Once there is an initial estimate of the camera pose and feature matching, covisibility graph used to get a local visible map.\n",
    "    * Search for matches in local map by reprojection; optimize camera pose again.\n",
    "    * Decide if new keyframe is to be inserted<br><br>\n",
    "  2. Local Mapping thread:\n",
    "    * Process new keyframes, perform local BA to get optimal reconstruction around camera pose.\n",
    "    * Find correspondences for the new unmatched ORB features in connected keyframes in the covisibility graph, triangulate new points.\n",
    "    * Point removal to retain high quality points.\n",
    "    * Redundant Keyframe culling.<br><br>\n",
    "  3. Loop Closure thread:\n",
    "    * Search for loop closure with every new keyframe.\n",
    "    * If loop detected, calculate similarity transform, (measure of drift accumulated in the loop).\n",
    "    * Align loops, fuse duplicate points.\n",
    "    * Pose graph optimization of __Essential Graph (sparser subset of covisibility graph)__\n",
    "    \n",
    "__C. Map points, keyframes and how to select:__\n",
    "  * *Each map point* $p_i$ stores:   \n",
    "    1. 3-D position in World Coordinate system: $X_{w,i}$  \n",
    "    2. Viewing direction: $n_i$, mean unit vector of all vieweing directions (rays that join point to optical center of observer keyframe)\n",
    "    3. ORB descriptor: $D_i$, minimum hamming distance out of all other descriptors from all the observer keyframes\n",
    "    4. Max and min distance at which a keypoint can be observed: $d_{max}$, $d_{min}$, wrt scale invariance limits of ORB<br><br>\n",
    "  \n",
    "  * *Each keyframe* $K_i$ stores:\n",
    "    1. Camera Pose: $T_{iw}$, rigid body transformation from World to Camera. Transformation of points from world to camera.\n",
    "    2. Camera intrinsics (including focal length and principal point).\n",
    "    3. All ORB featues extracted in the frame (undistored co-ordinates if distortion model provided).\n",
    "    \n",
    "__D. Covisibility Graph and Essential Graph:__\n",
    "  * Covisibility represented as an *undirected weighted* graph.\n",
    "    * Each node is a keyframe.\n",
    "    * Number of common map points, $\\theta$ is the weight of the edge between 2 keyframes. For an edge to exist, KFs should share observations of same (atleast 15) map points.<br><br>\n",
    "  * Essential Graph:\n",
    "    * Retains all nodes (keyframes) of Covisibilty graph but fewer edges; still preserves a strong network.\n",
    "    * Incrementally built spanning tree; connected subgraph of covisibility graph with minimum number of edges.\n",
    "    * High Covisibility edges are edges with $\\theta_{min} = 100$\n",
    "\n",
    "__E. Bag of Words Place Recog:__\n",
    "  * Incrementally update database of Invert Index. For each *visual word* in the vocab, we store the keyframe index where it was observed. Faster querying.\n",
    "  * Keyframes are grouped by connections in the Covisibility Graph.\n",
    "  * Multiple candidates returned with score > 0.75% of best score.\n",
    "  * This BoW embedding is also used for fidning correspondences. Brute force match constrained only to features that belong to same node of the vocab tree. Paper uses Second out of Six. Faster search.\n",
    "      * Refine correspondences with orientation consistency check, coherent rotation for all correspondences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Map Init\n",
    "__* Goal: To compute relative pose between two frames to traingulate an inital set of map points.*__<br>\n",
    "__* Should be scene independent (planar or non planar)*__<br>\n",
    "__* should not require human intervention.*__\n",
    "* Compute 2 geometric models. Use _a heuristic metric_ to select best model.\n",
    "    * **Homography**, assuming planar scene\n",
    "    * **Fundamental matrix**, assuming general (non planar) scene\n",
    "* Steps:\n",
    "    1. Find initial correspondences:\n",
    "        * Extract ORB feature (finest scale only) in Current Frame $F_c$ and references in Reference Frame $F_r$, $x_c \\leftrightarrow x_r$.\n",
    "        * If not enough matches, reset $F_r$\n",
    "    2. Parallel computuation of 2 models:\n",
    "        * For homogeneity between models, same RANSAC scheme for both, same # of iterations, same set of points for both models at each iteration.\n",
    "        * Homography $H_{cr}$\n",
    "            * Normalized DLT for Homography, $x_c = H_{cr}x_r$\n",
    "            * 4 point correspondences\n",
    "        * Fundamental Matrix $F_{cr}$\n",
    "            * Eight-point algorithm, ${x_c}^T F_{cr} x_r = 0$\n",
    "            * 8 correspondences, 4 of which are also used for Homography.\n",
    "        * Compute a score $S_M$ for each models (M is H and F)\n",
    "        * Keep homography AND fundamental matrix with highest score. We will then select in next step.\n",
    "    3. Model Selection:\n",
    "        * Calculate $R_H = \\large\\frac{S_H}{S_H + S_F}$\n",
    "        * Select homography if $R_H > 0.45$, else selct fundamental matrix\n",
    "    4. Motion and SfM recovery:\n",
    "        * Calculate motion hypothesis from the model selected in 3.\n",
    "        * If Homography, we get 8 motion hypothesis.\n",
    "            * Triangulate and select best reconstruction.\n",
    "        * If Fundamental Matrix:\n",
    "            * Get Essential Matrix, $E_{rc} = K^T F_{rc} K$\n",
    "            * Get 4 motion hypothesis with SVD\n",
    "            * Triangulate and select best reconstruction.\n",
    "    5. Bundle Adjustment:\n",
    "        * Full BA to refine initial reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "__* Done for every frame *__<br>\n",
    "__* Camera pose optimization is Motion-Only BA *__<br>\n",
    "\n",
    "**A. ORB Extraction:**\n",
    "  * Extract FAST corners at eight scale levels with a scale factor of 1.2.\n",
    "  * Compute ORB descriptors on the retained FAST corners.\n",
    "  \n",
    "**B. Initial pose estimation from previous frame:**\n",
    "  * **If tracking was successful** for last frame, use constant velocity model.\n",
    "  * Predict camera pose and perform guided search of map points seen in last frame.\n",
    "  * If not enough matches, the motion model is violated, use a wider search area.\n",
    "  * Optimize pose with found correspondences.\n",
    "  \n",
    "**C. Initial pose estimation via Global Relocalization:**\n",
    "  * **If tracking is lost**, convert frame to Bag of Words and query recognition DB for keyframe candidates for global reloc.\n",
    "  * Computer correspondences with ORB  for map points in each keyframe.\n",
    "  * Perform RANSAC with every keyframe and find camera pose using PnP.\n",
    "  * If pose found with enough keyframes perform guided search for more matches of map points in the candidate keyframe.\n",
    "  * Optimize camera pose.\n",
    "  \n",
    "**D. Track Local Map:**\n",
    "  * We now have Initial camera pose estimate and an initial set of features.\n",
    "  * Project the map into the frame and find more map point correspondences.\n",
    "  * **Use local map** to reduce complexity in large maps.\n",
    "  * The local map contains 3 types of Keyframes, $\\large\\kappa_i$\n",
    "    * $\\large\\kappa_1$: set of keyframes that share map points with current frame\n",
    "    * $\\large\\kappa_2$: neighbors of $\\large\\kappa_1$ keyframes from covisibility graph\n",
    "    * $K_{ref}$: $K_{ref} \\in \\large\\kappa_1$; keyframe with max number of shared map points with current frame\n",
    "  * Each map point in $\\large\\kappa_1$ and $\\large\\kappa_2$ is searched in current frame like this:\n",
    "    * Compute map point projection $\\mathbf{x}$ in current frame. Discard if out of image bounds.\n",
    "    * Compute angle between current viewing ray $\\mathbf{v}$ and map point mean viewing direction $\\mathbf{n}$. Discard if ${\\mathbf{v}}\\cdot{\\mathbf{n}} < \\cos(60 ^{\\circ})$$d$\n",
    "    * Compute distance $d$ from map point to camera center. Discard if outside scale invariance bounds $d \\notin [d_{min}, d_{max}]$. System Overview 'C'\n",
    "    * Computer Scale in frame as ratio $ \\large\\frac{d}{d_{min}}$\n",
    "    * Compare the descriptor $$\\mathbf{D}$ of the map point with the still unmatched ORB features in the frame, at the predicted scale, around $\\mathbf{x}$ and associate amp point with the best match.\n",
    "  * Optimize camera pose with all map points found in the frame.\n",
    "  \n",
    "**E. New Keyframe decision:**\n",
    "  * Since we cull keyframes later, we add keyframes as fast as possible.\n",
    "  * For the current frame to become a keyframe, following need to be true:\n",
    "      1. More than 20 frames have passed since last global reloc. **Ensures good reloc**\n",
    "      2. Local mapping is idle or more than 20 frames have passed since last keyframe creation.\n",
    "      3. Current frame tracks atelast 50 points. **Ensures good tracking**\n",
    "      4. Current frame tracks less than 90% points of $K_{ref}$. **Imposes a minimum visual change, instead of distance criterion from other keyframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Local Mapping\n",
    "__* Done for every new keyframe $K_i$ *__<br>\n",
    "\n",
    "**A. KeyFrame Insertion:**\n",
    "  * Update the Covisibility Graph:\n",
    "      * Add new node for $K_i$\n",
    "      * Upadte edges to other keyframes resulting from the shared map points\n",
    "  * Update spanning tree\n",
    "      * Link $K_i$ with the keyframe with most points in common.\n",
    "  * Compute a BoW representation for $K_i$. This will help in data association for triangulating new points.\n",
    "  \n",
    "**B. Recent Map Point Culling:**\n",
    "  * Test if a map point is keep-worthy during the first 3 keyframes after creation. Ensure a point is trackable and not wrongly triangulated.\n",
    "  * A map point must fulfill 2 conditions to avoid culling:\n",
    "      * Tracking should find a point in more than 25% of the frames in which it is predicted to be visible.\n",
    "      * If more than one keyframe has passed since creation, it must be observed from at least 3 keyframes.\n",
    "  * Once a map point passes a test, it can be removed only if it is observed from less than 3 keyframes.\n",
    "      * This might happen if a keyframe is culled later on\n",
    "      * Can also happen when local BA discards outlier observations.\n",
    "      \n",
    "**C. New Map Point Creation:**\n",
    "  * Triangulate ORB features from connected keyframes $\\large\\kappa_c$ in the Covisibility graph.\n",
    "      * Match unmatched ORB in $K_i$ with unmatched points in other $\\large\\kappa_c$ frames. Use the BoW for brute force pruning (III-E), then discard if epipolar constraint not satisfied.\n",
    "      * Upon triangulation, to accept new points check for **positive depth in both cameras, parallax, reprojection error and scale consistency**.\n",
    "      * A map point could be matched in multiple frames; project and search as in Tracking- D. Track local map\n",
    "      \n",
    "**D. Local Bundle Adjustment:**\n",
    "  * Local BA optimization of $K_i$, all connected keyframes in covisibility graph $\\large\\kappa_c$ and all map points seen in those keyframes\n",
    "  * All other keyframes that see those map points, but are not connected, are also added to optimization but act as anchors and remain fixed.\n",
    "  * Outlier observations are discarded in middle and end of optimization.\n",
    "  \n",
    "**E. Local Keyframe Culling:**\n",
    "  * Redundant keyframes need to be discarded to enable lifelong operation. we want keyframes to be added only when there is enough visual content change.\n",
    "  * Remove all keyframes in $\\large\\kappa_c$ where more than 90% of its points can be seen by atleast 3 other keyframes in the same or finer scale. The finer the scale, the more accurate the depth information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
